{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a8177d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pylab import *\n",
    "from pyspark.sql.functions import udf, concat, col, lit\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MyApp\")\n",
    "\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "except:\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1387cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load('waterquality.csv')\n",
    "\n",
    "gdf = gpd.read_file('Indian_States.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fee2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+----+---+---+------------+---+-------------------+--------------+--------------+\n",
      "|STATION CODE|           LOCATIONS|      STATE|TEMP| DO| pH|CONDUCTIVITY|BOD|NITRATE_N_NITRITE_N|FECAL_COLIFORM|TOTAL_COLIFORM|\n",
      "+------------+--------------------+-----------+----+---+---+------------+---+-------------------+--------------+--------------+\n",
      "|        1312|GODAVARI AT JAYAK...|MAHARASHTRA|29.2|6.4|8.1|         735|3.4|                  2|             3|            73|\n",
      "|        2177|GODAVARI RIVER NE...|MAHARASHTRA|24.5|  6|  8|         270|3.1|                  2|            72|           182|\n",
      "|        2182|GODAVARI RIVER AT...|MAHARASHTRA|25.8|5.5|7.8|         355|4.2|                  9|            59|           133|\n",
      "|        2179|GODAVARI RIVER AT...|MAHARASHTRA|24.8|5.5|7.8|         371|5.6|               3.55|            90|           283|\n",
      "|        2183|GODAVARI RIVER AT...|MAHARASHTRA|25.7|5.7|7.9|         294|3.2|               2.69|            45|           132|\n",
      "+------------+--------------------+-----------+----+---+---+------------+---+-------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946244c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('STATION CODE', 'string'),\n",
       " ('LOCATIONS', 'string'),\n",
       " ('STATE', 'string'),\n",
       " ('TEMP', 'string'),\n",
       " ('DO', 'string'),\n",
       " ('pH', 'string'),\n",
       " ('CONDUCTIVITY', 'string'),\n",
       " ('BOD', 'string'),\n",
       " ('NITRATE_N_NITRITE_N', 'string'),\n",
       " ('FECAL_COLIFORM', 'string'),\n",
       " ('TOTAL_COLIFORM', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ef9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d9d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('STATION CODE', 'string'),\n",
       " ('LOCATIONS', 'string'),\n",
       " ('STATE', 'string'),\n",
       " ('TEMP', 'float'),\n",
       " ('DO', 'float'),\n",
       " ('pH', 'float'),\n",
       " ('CONDUCTIVITY', 'float'),\n",
       " ('BOD', 'float'),\n",
       " ('NITRATE_N_NITRITE_N', 'float'),\n",
       " ('FECAL_COLIFORM', 'float'),\n",
       " ('TOTAL_COLIFORM', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"TEMP\",df[\"TEMP\"].cast(FloatType()))\n",
    "df = df.withColumn(\"pH\",df[\"pH\"].cast(FloatType()))\n",
    "df = df.withColumn(\"DO\",df[\"DO\"].cast(FloatType()))\n",
    "df = df.withColumn(\"CONDUCTIVITY\",df[\"CONDUCTIVITY\"].cast(FloatType()))\n",
    "df = df.withColumn(\"BOD\",df[\"BOD\"].cast(FloatType()))\n",
    "df = df.withColumn(\"NITRATE_N_NITRITE_N\",df[\"NITRATE_N_NITRITE_N\"].cast(FloatType()))\n",
    "df = df.withColumn(\"FECAL_COLIFORM\",df[\"FECAL_COLIFORM\"].cast(FloatType()))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "776fbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('TOTAL_COLIFORM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d184f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd9609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = spark.sql('''Select * from df_sql where TEMP is not null and DO is not null \n",
    "                        and pH is not null and BOD is not null and CONDUCTIVITY is not null\n",
    "                        and NITRATE_N_NITRITE_N is not null and FECAL_COLIFORM is not null''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcff5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.createOrReplaceTempView(\"df_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f56dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------+----+---+---+------------+----+-------------------+--------------+\n",
      "|STATION CODE|           LOCATIONS|         STATE|TEMP| DO| pH|CONDUCTIVITY| BOD|NITRATE_N_NITRITE_N|FECAL_COLIFORM|\n",
      "+------------+--------------------+--------------+----+---+---+------------+----+-------------------+--------------+\n",
      "|        1312|GODAVARI AT JAYAK...|   MAHARASHTRA|29.2|6.4|8.1|       735.0| 3.4|                2.0|           3.0|\n",
      "|        2177|GODAVARI RIVER NE...|   MAHARASHTRA|24.5|6.0|8.0|       270.0| 3.1|                2.0|          72.0|\n",
      "|        2182|GODAVARI RIVER AT...|   MAHARASHTRA|25.8|5.5|7.8|       355.0| 4.2|                9.0|          59.0|\n",
      "|        2179|GODAVARI RIVER AT...|   MAHARASHTRA|24.8|5.5|7.8|       371.0| 5.6|               3.55|          90.0|\n",
      "|        2183|GODAVARI RIVER AT...|   MAHARASHTRA|25.7|5.7|7.9|       294.0| 3.2|               2.69|          45.0|\n",
      "|        2181|GODAVARI RIVER AT...|   MAHARASHTRA|25.0|4.5|7.5|       513.0|12.6|                2.3|         131.0|\n",
      "|        2180|GODAVARI RIVER NE...|   MAHARASHTRA|24.8|5.2|7.7|       475.0|10.3|                1.9|         122.0|\n",
      "|        1096|GODAVARI AT PANCH...|   MAHARASHTRA|26.3|5.6|7.7|       385.0| 3.8|                1.0|         110.0|\n",
      "|        1211|GODAVARI AT NASIK...|   MAHARASHTRA|26.5|5.2|7.8|       410.0| 5.2|                1.5|          77.0|\n",
      "|        1095|GODAVARI AT U/S O...|   MAHARASHTRA|26.0|6.5|7.8|       178.0| 2.5|               1.99|          22.0|\n",
      "|        2160|GODAVARI RIVER AT...|   MAHARASHTRA|29.2|6.2|7.9|       749.0| 3.5|               1.75|           2.0|\n",
      "|        2158|GODAVARI RIVER AT...|   MAHARASHTRA|29.5|6.4|8.0|       834.0| 4.7|                1.8|           2.0|\n",
      "|        2159|GODAVARI RIVER AT...|   MAHARASHTRA|30.2|6.2|8.0|       925.0| 4.2|                6.9|           2.0|\n",
      "|        2161|GODAVARI RIVER AT...|   MAHARASHTRA|26.7|6.5|8.0|       730.0| 4.0|                2.9|           2.0|\n",
      "|          12|GODAVARI AT DHALE...|   MAHARASHTRA|27.2|7.0|7.9|       637.0| 4.7|                6.5|           2.0|\n",
      "|        1209|GODAVARI AT RAHER...|   MAHARASHTRA|29.7|6.4|8.0|       656.0| 3.6|                1.8|           3.0|\n",
      "|        2157|GODAVARI RIVER AT...|   MAHARASHTRA|26.0|6.6|8.1|       601.0| 3.2|               6.75|           2.0|\n",
      "|        2360|GODAVARI AT BASAR...|ANDHRA PRADESH|28.0|5.5|8.1|       826.0| 1.7|                1.0|          27.0|\n",
      "|        2361|GODAVARI��AT MANC...|ANDHRA PRADESH|31.0|6.9|9.0|       526.0|12.0|                0.0|         130.0|\n",
      "|        2362|GODAVARI AT RAMAG...|ANDHRA PRADESH|30.0|5.7|8.5|       575.0|13.0|                0.0|         240.0|\n",
      "+------------+--------------------+--------------+----+---+---+------------+----+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d08d3934",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 11) (shivam-pc executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m do \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect DO from df_sql\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m do \u001b[38;5;241m=\u001b[39m \u001b[43mdo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDO\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ph \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect pH from df_sql\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m ph \u001b[38;5;241m=\u001b[39m ph\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row : row\u001b[38;5;241m.\u001b[39mpH)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 11) (shivam-pc executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "do = spark.sql(\"Select DO from df_sql\")\n",
    "do = do.rdd.map(lambda row : row.DO).collect()\n",
    "ph = spark.sql(\"Select pH from df_sql\")\n",
    "ph = ph.rdd.map(lambda row : row.pH).collect()\n",
    "bod = spark.sql(\"Select BOD from df_sql\")\n",
    "bod = bod.rdd.map(lambda row : row.BOD).collect()\n",
    "nn = spark.sql(\"Select NITRATE_N_NITRITE_N from df_sql\")\n",
    "nn = nn.rdd.map(lambda row : row.NITRATE_N_NITRITE_N).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3cd8d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'DataFrame' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m6\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,size), do, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, animated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDissolved Oxygen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,size), ph, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, animated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'DataFrame' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5YAAAGXCAYAAADIyM/yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYeklEQVR4nO3dX2id9f3A8c/RMHrRiV6Y2jY9PWttirWtjUq1VcTgmJ3YMskmBesfTEhlFZFAC5sUNudEdhHwykYoWuissNZu9UYv1DkLGzrisJpJ05mYHCVGhhDjVtey53chv2z5tZ3P+olJ+vP1ggM++OnhE/gSeHOe56RSFEURAAAAcJbOm+kFAAAAOLcJSwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkFIqLB944IGo1WpRqVTi7bffPuPc7t27Y9myZbF06dLo7OyMkydPTtmiAAAAzE6lwvL73/9+HD58OBYvXnzGmYGBgdi5c2ccPnw4jh07FiMjI7F79+4pWxQAAIDZqVRY3nDDDdHU1PQfZ/bv3x+33XZbzJs3LyqVStx3332xb9++KVkSAACA2WvKnrEcGhqa9IlmrVaLoaGhqXp7AAAAZqmGqXyzSqUy8d9FUfzH2e7u7uju7p64HhkZiUsuuWQq1wEAAKCEjz/+OD7//POz/vdTFpbVajUGBwcnrt9///2oVqtnnO/q6oqurq6J66ampqjX61O1DgAAACV92aOPX2bKboVta2uLgwcPxkcffRRFUcSuXbti8+bNU/X2AAAAzFKlwnLbtm0Tnyh++9vfjksvvTQiIjo6OuLQoUMREbFkyZL46U9/Gtddd10sXbo0Ghsbo729/avbHAAAgFmhUnzZw5DTxK2wAAAAMyPbY1N2KywAAABfT8ISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACmlw7K/vz/Wr18fzc3NsXbt2ujr6ztlpiiK2L59e1x++eWxevXqaG1tjWPHjk3pwgAAAMwupcNy69at0dnZGUePHo0dO3ZEe3v7KTOHDh2K3/3ud/GnP/0p3nrrrbjpppvixz/+8ZQuDAAAwOxSKixHR0ejt7c3tmzZEhERbW1tMTAwEIODg6fMfv7553H8+PEoiiLGxsaiqalpShcGAABgdmkoMzQ8PBwLFiyIhoYvxiuVSlSr1RgaGoparTYxt3Hjxvjtb38bl1xySXzzm9+MhQsXxquvvvqVLA4AAMDsUPpW2EqlMum6KIpTZnp7e+Pdd9+NDz74ID788MO46aab4v777z/t+3V3d0dTU9PEa3x8/L9cHQAAgNmgVFguWrQo6vV6nDx5MiK+iMrh4eGoVquT5p5++ulobW2NCy+8MM4777y4++6745VXXjnte3Z1dUW9Xp94zZ07N/mjAAAAMBNKhWVjY2O0tLTE3r17IyLiwIEDUavVJt0GGxGxZMmSeOmll+LEiRMREfH888/HypUrp3ZjAAAAZpVSz1hGRPT09MQ999wTjz76aFxwwQWxZ8+eiIjo6OiITZs2xaZNm2Lbtm3x5z//OVatWhXf+MY3Yv78+dHT0/OVLQ8AAMDMqxSne1hyBjQ1NUW9Xp/pNQAAAL52sj1W+st7AAAA4HSEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABSSodlf39/rF+/Ppqbm2Pt2rXR19d32rkjR47EjTfeGJdddlksX748nnvuuSlbFgAAgNmnoezg1q1bo7OzM+65557Yv39/tLe3x+9///tJM3/729/ie9/7XuzZsyeuv/76OHnyZHzyySdTvjQAAACzR6lPLEdHR6O3tze2bNkSERFtbW0xMDAQg4ODk+aeeeaZWLduXVx//fUREdHQ0BAXX3zx1G4MAADArFIqLIeHh2PBggXR0PDFB5yVSiWq1WoMDQ1Nmuvr64s5c+bErbfeGmvWrIm77rorPv7449O+Z3d3dzQ1NU28xsfHkz8KAAAAM6H0M5aVSmXSdVEUp8ycOHEiXnzxxejp6Yk333wzFi1aFNu2bTvt+3V1dUW9Xp94zZ07979cHQAAgNmgVFguWrQo6vV6nDx5MiK+iMrh4eGoVquT5hYvXhytra2xcOHCqFQqcccdd8Trr78+9VsDAAAwa5QKy8bGxmhpaYm9e/dGRMSBAweiVqtFrVabNHf77bfHG2+8EWNjYxER8cILL8QVV1wxtRsDAAAwq5T+Vtienp6455574tFHH40LLrgg9uzZExERHR0dsWnTpti0aVNUq9X40Y9+FOvWrYuGhoZYuHBhPPnkk1/Z8gAAAMy8SnG6hyVnQFNTU9Tr9ZleAwAA4Gsn22Olv7wHAAAATkdYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACCldFj29/fH+vXro7m5OdauXRt9fX1nnD1+/HisWLEirr766ilZEgAAgNmrdFhu3bo1Ojs74+jRo7Fjx45ob28/4+xDDz0U69atm5IFAQAAmN1KheXo6Gj09vbGli1bIiKira0tBgYGYnBw8JTZ1157Lfr7++POO++c0kUBAACYnUqF5fDwcCxYsCAaGhoiIqJSqUS1Wo2hoaFJc5999lk8+OCD8cQTT3zpe3Z3d0dTU9PEa3x8/CzWBwAAYKaVvhW2UqlMui6K4pSZ7du3x7Zt22LhwoVf+n5dXV1Rr9cnXnPnzi27CgAAALNIpThdIf4fo6OjsWzZsvjrX/8aDQ0NURRFzJ8/P/7whz9ErVabmFu9enWMjY1FxBdf4PPJJ5/EpZdeGu+8886XLtLU1BT1ev3sfxIAAADOSrbHSn1i2djYGC0tLbF3796IiDhw4EDUarVJURkR8dZbb8Xg4GAMDg7Gs88+G6tWrSoVlQAAAJy7St8K29PTEz09PdHc3ByPPfZY7N69OyIiOjo64tChQ1/ZggAAAMxupW6FnQ5uhQUAAJgZ03IrLAAAAJyJsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABASumw7O/vj/Xr10dzc3OsXbs2+vr6Tpl5+eWX45prrokVK1bEypUr46GHHoqiKKZ0YQAAAGaX0mG5devW6OzsjKNHj8aOHTuivb39lJmLLroo9u3bF319ffHHP/4xXn311di3b9+ULgwAAMDsUiosR0dHo7e3N7Zs2RIREW1tbTEwMBCDg4OT5lpaWmLJkiURETFnzpxYs2ZNvPfee1O7MQAAALNKqbAcHh6OBQsWRENDQ0REVCqVqFarMTQ0dMZ/MzIyEvv3749bbrnltP+/u7s7mpqaJl7j4+NnsT4AAAAzrfStsJVKZdL1f3p2cmxsLDZu3Bg7duyIK6+88rQzXV1dUa/XJ15z584tuwoAAACzSKmwXLRoUdTr9Th58mREfBGVw8PDUa1WT5n99NNPY8OGDbFp06bo6uqa2m0BAACYdUqFZWNjY7S0tMTevXsjIuLAgQNRq9WiVqtNmhsfH48NGzbEzTffHDt37pzyZQEAAJh9St8K29PTEz09PdHc3ByPPfZY7N69OyIiOjo64tChQxER8fjjj8frr78eBw8ejDVr1sSaNWvi5z//+VezOQAAALNCpZglf2iyqakp6vX6TK8BAADwtZPtsdKfWAIAAMDpCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAFGEJAABAirAEAAAgRVgCAACQIiwBAABIEZYAAACkCEsAAABShCUAAAApwhIAAIAUYQkAAECKsAQAACBFWAIAAJAiLAEAAEgRlgAAAKQISwAAAFKEJQAAACnCEgAAgBRhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApJQOy/7+/li/fn00NzfH2rVro6+v77Rzu3fvjmXLlsXSpUujs7MzTp48OWXLAgAAMPuUDsutW7dGZ2dnHD16NHbs2BHt7e2nzAwMDMTOnTvj8OHDcezYsRgZGYndu3dP6cIAAADMLqXCcnR0NHp7e2PLli0REdHW1hYDAwMxODg4aW7//v1x2223xbx586JSqcR9990X+/btm/KlAQAAmD0aygwNDw/HggULoqHhi/FKpRLVajWGhoaiVqtNzA0NDcXixYsnrmu1WgwNDZ32Pbu7u6O7u3vi+sMPP4ympqaz+RlgRoyPj8fcuXNneg34rzi3nGucWc5Fzi3nopGRkdS/LxWWEV/E5L8riuJL5840ExHR1dUVXV1dE9dNTU1Rr9fLrgMzzpnlXOTccq5xZjkXObeci7If8pW6FXbRokVRr9cnvoinKIoYHh6OarU6aa5arU66Pfb9998/ZQYAAID/X0qFZWNjY7S0tMTevXsjIuLAgQNRq9Um3QYb8cWzlwcPHoyPPvooiqKIXbt2xebNm6d8aQAAAGaP83/yk5/8pMzgunXrYufOnfGLX/wi3njjjXjqqaeisbExOjo6IiJi+fLlcdFFF8UFF1wQ9957bzz++OOxcuXK2LlzZ5x//vmlllm3bt1Z/yAwE5xZzkXOLecaZ5ZzkXPLuShzbivFf3oQEgAAAL5E6b9jCQAAAKcjLAEAAEiZtrDs7++P9evXR3Nzc6xduzb6+vpOO7d79+5YtmxZLF26NDo7Oye+iRZmQplz+/LLL8c111wTK1asiJUrV8ZDDz30H//UDnzVyv6+jYg4fvx4rFixIq6++upp3BAmK3tmjxw5EjfeeGNcdtllsXz58njuueemeVP4lzLntiiK2L59e1x++eWxevXqaG1tjWPHjs3AthDxwAMPRK1Wi0qlEm+//fYZ5866x4pp0traWjz11FNFURTFr371q+Laa689Zea9994r5s+fX4yMjBT//Oc/i40bNxa7du2arhXhFGXObW9vb/GXv/ylKIqi+Pvf/15cd911xS9/+cvpXBMmKXNu/1dXV1dx7733FlddddU0bQenKnNmP/vss2LJkiXFa6+9VhRFUZw4caIYHR2dzjVhkjLn9te//nWxdu3a4h//+EdRFEXxs5/9rPjBD34wnWvChFdffbUYHh4uFi9eXBw5cuS0M5kem5ZPLEdHR6O3tze2bNkSEV/8WZKBgYFJf/MyImL//v1x2223xbx586JSqcR9990X+/btm44V4RRlz21LS0ssWbIkIiLmzJkTa9asiffee2+614WIKH9uIyJee+216O/vjzvvvHOat4R/KXtmn3nmmVi3bl1cf/31ERHR0NAQF1988XSvCxHx3/2u/fzzz+P48eNRFEWMjY2l/wg9nK0bbrjhS89fpsemJSyHh4djwYIF0dDQEBERlUolqtVqDA0NTZobGhqKxYsXT1zXarVTZmC6lD23/25kZCT2798ft9xyy3StCZOUPbefffZZPPjgg/HEE0/MxJowoeyZ7evrizlz5sStt94aa9asibvuuis+/vjjmVgZSp/bjRs3Rmtra1xyySUxf/78eOmll+Lhhx+eiZWhlEyPTdszlpVKZdJ1cYZn0P597kwzMF3KntuIiLGxsdi4cWPs2LEjrrzyyq96NTijMud2+/btsW3btli4cOF0rQVnVObMnjhxIl588cXo6emJN998MxYtWhTbtm2brhXhFGXObW9vb7z77rvxwQcfxIcffhg33XRT3H///dO1IpyVs+2xaQnLRYsWRb1en3jwsyiKGB4ejmq1OmmuWq1OuoXg/fffP2UGpkvZcxsR8emnn8aGDRti06ZN0dXVNd2rwoSy5/bw4cPx8MMPR61Wi82bN8eRI0fi8ssvn4mV+Zore2YXL14cra2tsXDhwqhUKnHHHXfE66+/PhMrQ+lz+/TTT0dra2tceOGFcd5558Xdd98dr7zyykysDKVkemxawrKxsTFaWlpi7969ERFx4MCBqNVqUavVJs21tbXFwYMH46OPPoqiKGLXrl2xefPm6VgRTlH23I6Pj8eGDRvi5ptvjp07d87ApvAvZc/tW2+9FYODgzE4OBjPPvtsrFq1Kt55550Z2Jivu7Jn9vbbb4833ngjxsbGIiLihRdeiCuuuGK614WIKH9ulyxZEi+99FKcOHEiIiKef/75WLly5XSvC6Wleiz11UL/hXfffbe49tpri2XLlhVXXXVV8fbbbxdFURTt7e3Fb37zm4m5J598sli6dGnxrW99q2hvb5/4Fi2YCWXO7SOPPFI0NDQUV1xxxcTrkUcemcm1+Zor+/v2f73yyiu+FZYZVfbM7tmzp1ixYkWxevXq4rvf/W4xPDw8UytDqXN7/PjxoqOjo1i+fHmxatWq4jvf+U4xMDAwg1vzdfbDH/6wWLhwYXH++ecX8+bNK5YuXVoUxdT1WKUoPMgIAADA2Zu2L+8BAADg/ydhCQAAQIqwBAAAIEVYAgAAkCIsAQAASBGWAAAApAhLAAAAUoQlAAAAKcISAACAlP8B3JktXEsmbvgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1120x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size=len(do)\n",
    "ax.plot(range(0,size), do, color='blue', animated=True, linewidth=1, label='Dissolved Oxygen')\n",
    "ax.plot(range(0,size), ph, color='red', animated=True, linewidth=1, label='pH')\n",
    "fig,ax2 = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax2.plot(range(0,size), bod, color='orange', animated=True, linewidth=1, label='BOD')\n",
    "ax2.plot(range(0,size), nn, color='green', animated=True, linewidth=1, label='NN')\n",
    "legend=ax.legend()\n",
    "legend=ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba257ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
